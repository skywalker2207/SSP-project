{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import scipy.io.wavfile as wave\n",
    "from scipy.fftpack import fft, ifft, fftshift\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sklearn import mixture\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import json\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tarun/snap/SSP-project\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_mixtures = 8\n",
    "max_iterations = 1000\n",
    "#calc_deltas=True\n",
    "#sr=8000\n",
    "#hop_length=int(0.005*sr)\n",
    "home_path = os.getcwd()\n",
    "#path = os.path.join(home_path, 'data')\n",
    "print(home_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file):\n",
    "    feature = loadmat(file)\n",
    "    \n",
    "    dur_list = np.array([[element for element in upperElement] for upperElement in feature['dur']])\n",
    "    f0_list = np.array([[element for element in upperElement] for upperElement in feature['f0']])\n",
    "    amp_tilt_list = np.array([[element for element in upperElement] for upperElement in feature['amp_tilt']])\n",
    "    dur_tilt_list = np.array([[element for element in upperElement] for upperElement in feature['dur_tilt']])\n",
    "    differ_list = np.array([[element for element in upperElement] for upperElement in feature['differ']])\n",
    "    voiced_dur_list = np.array([[element for element in upperElement] for upperElement in feature['voiced_dur']])\n",
    "    log_energy_list = np.array([[element for element in upperElement] for upperElement in feature['log_energy']])\n",
    "    \n",
    "    dur_list = dur_list.ravel()\n",
    "    f0_list = f0_list.ravel()\n",
    "    amp_tilt_list = amp_tilt_list.ravel()\n",
    "    dur_tilt_list = dur_tilt_list.ravel()\n",
    "    differ_list = differ_list.ravel()\n",
    "    voiced_dur_list = voiced_dur_list.ravel()\n",
    "    log_energy_list = log_energy_list.ravel()\n",
    "    \n",
    "    feature_vec = np.vstack((dur_list,f0_list,amp_tilt_list,dur_tilt_list,differ_list,voiced_dur_list,log_energy_list))\n",
    "    feature_vec = np.nan_to_num(feature_vec)\n",
    "    feature_vec = feature_vec.T\n",
    "    return feature_vec\n",
    "    #print(feature_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_data_path,feat_train_path,trained_model_path):\n",
    "    all_speakers=glob.glob(train_data_path+'*')\n",
    "    #print(all_speakers)\n",
    "    print(2)\n",
    "\n",
    "    directory=feat_train_path\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "      \n",
    "    directory=trained_model_path\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    print(len(all_speakers))\n",
    "    for itr1 in range(0,len(all_speakers)):\n",
    "        \n",
    "        mats=glob.glob(all_speakers[itr1]+'/*.mat')\n",
    "        spk=(all_speakers[itr1]).split(\"/\")[-1]\n",
    "        #print((mats))\n",
    "\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "      \n",
    "        final_feat=np.empty([0, 35])\n",
    "        \n",
    "        for itr2 in range(0,len(mats)):\n",
    "  \n",
    "            #final_feat = eng.prosody(mats[itr2])\n",
    "            #mat file\n",
    "            #mats[itr2]=mats[itr2].replace(\"\\\\\",\"/\")\n",
    "            #print(mats[itr2])\n",
    "            final_feat = extract_features(mats[itr2])\n",
    "            #print(final_feat.shape)\n",
    "\n",
    "        #print(spk)    \n",
    "        np.savetxt(feat_train_path + spk + \"_all_features.txt\" , final_feat , delimiter=\",\")\n",
    "        try:\n",
    "            gmm = mixture.GaussianMixture(n_components=n_mixtures, covariance_type='diag' , max_iter = max_iterations ).fit(final_feat)\n",
    "        except:\n",
    "            print(\"ERROR : Error while training model for file \"+spk+mats[itr2])\n",
    "          \n",
    "        try:\n",
    "            joblib.dump(gmm,trained_model_path+spk+'.pkl')\n",
    "        except:\n",
    "            print(\"ERROR : Error while saving model for \"+spk)\n",
    "          \n",
    "\n",
    "    print(\"Training Completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(test_data_path,feat_test,trained_model_path):\n",
    "    # train feature extraction\n",
    "    all_speakers=glob.glob(test_data_path+'*')\n",
    "\n",
    "    import os\n",
    "    directory=feat_test\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    speakers = { all_speakers[k]:k for k in range(len(all_speakers)) }\n",
    "    print(speakers) \n",
    "   \n",
    "    num_test_cases={}\n",
    "    tct={}\n",
    "    for e in speakers:\n",
    "        num_test_cases[e.replace(test_data_path,'')]=len(os.listdir(e))-1\n",
    "        tct[e.replace(test_data_path,'')]=0\n",
    "\n",
    "    # print(num_test_cases)\n",
    "\n",
    "    spk_names = { all_speakers[k].replace(test_data_path,''):k for k in range(len(all_speakers)) }\n",
    "    total_speakers=len(num_test_cases)\n",
    "    confusion_matrix = np.zeros((total_speakers,total_speakers))\n",
    "\n",
    "\n",
    "    for itr1 in range(0,len(all_speakers)):\n",
    "        mats=glob.glob(all_speakers[itr1]+'/*.mat')\n",
    "        spk=(all_speakers[itr1]).split(\"/\")[-1]\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            \n",
    "        final_feat=np.empty([0, 35])\n",
    "        output = {\"Test_i\":[],\"Accent\":[]}\n",
    "\n",
    "        for itr2 in range(0,len(mats)):\n",
    "            print(mats[itr2])\n",
    "\n",
    "            feat = extract_features(mats[itr2])\n",
    "            #print(feat)\n",
    "            max_score=-np.inf\n",
    "            max_spk_name=\"\"\n",
    "\n",
    "            for modelfile in sorted(glob.glob(trained_model_path+'*.pkl')):\n",
    "                gmm = joblib.load(modelfile) \n",
    "                score=gmm.score(feat)\n",
    "                #print score\n",
    "                if score>max_score:\n",
    "                    max_score,max_spk_name=score,modelfile.replace(trained_model_path,'').replace('.pkl','')\n",
    "            try:\n",
    "                print(spk+\" -> \"+max_spk_name+(\" Y\" if spk==max_spk_name  else \" N\"))\n",
    "                output [\"Test_i\"].append(mats[itr2])\n",
    "                output [\"Accent\"].append(max_spk_name)\n",
    "                confusion_matrix[ spk_names[spk] ][spk_names[max_spk_name]]+=1\n",
    "                tct[spk]+=1\n",
    "            except:\n",
    "                print(\"ERROR : Error while testing file \"+spk+mats[itr2])\n",
    "\n",
    "        #print(spk)\n",
    "        json_object = json.dumps(output, indent = 4)\n",
    "\n",
    "        with open(\"Output.json\", \"w\") as outfile:\n",
    "              outfile.write(json_object)\n",
    "        np.savetxt(feat_test+spk+\"_all_features.txt\", feat, delimiter=\",\")\n",
    "        \n",
    "    return tct,confusion_matrix,total_speakers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./feat/\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "feat= './feat/'\n",
    "feat_train=  './feat/train/'\n",
    "feat_test= './feat/test/'\n",
    "trained_model= './trained_model/'\n",
    "train_data= './traindata/'\n",
    "test_data= './testdata/'\n",
    "print(feat)\n",
    "# for removing existing feature folders, models created\n",
    "if os.path.exists('./feat/'):\n",
    "  !rm -rf  './feat/'\n",
    "if os.path.exists('./feat/test/'):\n",
    "  !rm -rf  './feat/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "7\n",
      "ERROR : Error while training model for file bengali./traindata/bengali/bengalif25.mat\n",
      "Training Completed\n"
     ]
    }
   ],
   "source": [
    "training(train_data,feat_train,trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'./testdata/manipur': 0, './testdata/telugu': 1, './testdata/bengali': 2, './testdata/odia': 3, './testdata/assam': 4, './testdata/gujrati': 5, './testdata/marathi': 6}\n",
      "./testdata/manipur/manipurf6.mat\n",
      "manipur -> manipur Y\n",
      "./testdata/manipur/manipurf7.mat\n",
      "manipur -> manipur Y\n",
      "./testdata/manipur/manipurf2.mat\n",
      "manipur -> manipur Y\n",
      "./testdata/manipur/manipurf1.mat\n",
      "manipur -> manipur Y\n",
      "./testdata/manipur/manipurf8.mat\n",
      "manipur -> manipur Y\n",
      "./testdata/manipur/manipurf9.mat\n",
      "manipur -> manipur Y\n",
      "./testdata/manipur/manipurf10.mat\n",
      "manipur -> manipur Y\n",
      "./testdata/manipur/manipurf3.mat\n",
      "manipur -> manipur Y\n",
      "./testdata/manipur/manipurf11.mat\n",
      "manipur -> manipur Y\n",
      "./testdata/manipur/manipurf5.mat\n",
      "manipur -> manipur Y\n",
      "./testdata/telugu/telugu9.mat\n",
      "telugu -> bengali N\n",
      "./testdata/telugu/telugu5.mat\n",
      "telugu -> manipur N\n",
      "./testdata/telugu/telugu6.mat\n",
      "telugu -> bengali N\n",
      "./testdata/telugu/telugu8.mat\n",
      "telugu -> bengali N\n",
      "./testdata/telugu/telugu10.mat\n",
      "telugu -> odia N\n",
      "./testdata/telugu/telugu1.mat\n",
      "telugu -> bengali N\n",
      "./testdata/telugu/telugu2.mat\n",
      "telugu -> bengali N\n",
      "./testdata/telugu/telugu3.mat\n",
      "telugu -> odia N\n",
      "./testdata/telugu/telugu7.mat\n",
      "telugu -> odia N\n",
      "./testdata/telugu/telugu4.mat\n",
      "telugu ->  N\n",
      "ERROR : Error while testing file telugu./testdata/telugu/telugu4.mat\n",
      "./testdata/bengali/bengalif9.mat\n",
      "bengali -> manipur N\n",
      "./testdata/bengali/bengalif10.mat\n",
      "bengali -> manipur N\n",
      "./testdata/bengali/bengalif6.mat\n",
      "bengali -> manipur N\n",
      "./testdata/bengali/bengalif2.mat\n",
      "bengali -> bengali Y\n",
      "./testdata/bengali/bengalif4.mat\n",
      "bengali -> bengali Y\n",
      "./testdata/bengali/bengalif3.mat\n",
      "bengali -> bengali Y\n",
      "./testdata/bengali/bengalif5.mat\n",
      "bengali -> bengali Y\n",
      "./testdata/bengali/bengalif1.mat\n",
      "bengali -> bengali Y\n",
      "./testdata/bengali/bengalif8.mat\n",
      "bengali -> odia N\n",
      "./testdata/bengali/bengalif7.mat\n",
      "bengali -> manipur N\n",
      "./testdata/odia/odiaf4.mat\n",
      "odia -> odia Y\n",
      "./testdata/odia/odiaf8.mat\n",
      "odia -> odia Y\n",
      "./testdata/odia/odiaf5.mat\n",
      "odia -> odia Y\n",
      "./testdata/odia/odiaf9.mat\n",
      "odia -> odia Y\n",
      "./testdata/odia/odiaf7.mat\n",
      "odia -> odia Y\n",
      "./testdata/odia/odiaf6.mat\n",
      "odia -> odia Y\n",
      "./testdata/odia/odiaf3.mat\n",
      "odia ->  N\n",
      "ERROR : Error while testing file odia./testdata/odia/odiaf3.mat\n",
      "./testdata/odia/odiaf1.mat\n",
      "odia -> odia Y\n",
      "./testdata/assam/assamf6.mat\n",
      "assam -> odia N\n",
      "./testdata/assam/assamf10.mat\n",
      "assam -> odia N\n",
      "./testdata/assam/assamf4.mat\n",
      "assam -> odia N\n",
      "./testdata/assam/assamf2.mat\n",
      "assam -> odia N\n",
      "./testdata/assam/assamf3.mat\n",
      "assam ->  N\n",
      "ERROR : Error while testing file assam./testdata/assam/assamf3.mat\n",
      "./testdata/assam/assamf9.mat\n",
      "assam -> odia N\n",
      "./testdata/assam/assamf7.mat\n",
      "assam -> odia N\n",
      "./testdata/assam/assamf8.mat\n",
      "assam -> odia N\n",
      "./testdata/assam/assamf5.mat\n",
      "assam -> odia N\n",
      "./testdata/assam/assamf1.mat\n",
      "assam -> odia N\n",
      "./testdata/gujrati/gujrat28.mat\n",
      "gujrati -> odia N\n",
      "./testdata/gujrati/gujrat24.mat\n",
      "gujrati -> odia N\n",
      "./testdata/gujrati/gujrat19.mat\n",
      "gujrati -> manipur N\n",
      "./testdata/gujrati/gujrat25.mat\n",
      "gujrati -> manipur N\n",
      "./testdata/gujrati/gujrat22.mat\n",
      "gujrati -> manipur N\n",
      "./testdata/gujrati/gujrat21.mat\n",
      "gujrati -> manipur N\n",
      "./testdata/gujrati/gujrat20.mat\n",
      "gujrati -> manipur N\n",
      "./testdata/gujrati/gujrat23.mat\n",
      "gujrati -> manipur N\n",
      "./testdata/gujrati/gujrat26.mat\n",
      "gujrati -> odia N\n",
      "./testdata/gujrati/gujrat27.mat\n",
      "gujrati -> manipur N\n",
      "./testdata/marathi/marathif7.mat\n",
      "marathi -> manipur N\n",
      "./testdata/marathi/marathif1.mat\n",
      "marathi -> manipur N\n",
      "./testdata/marathi/marathif6.mat\n",
      "marathi -> manipur N\n",
      "./testdata/marathi/marathif2.mat\n",
      "marathi -> manipur N\n",
      "./testdata/marathi/marathif5.mat\n",
      "marathi -> odia N\n",
      "./testdata/marathi/marathif4.mat\n",
      "marathi -> bengali N\n",
      "./testdata/marathi/marathif3.mat\n",
      "marathi -> manipur N\n",
      "./testdata/marathi/marathif8.mat\n",
      "marathi -> manipur N\n",
      "./testdata/marathi/marathif9.mat\n",
      "marathi -> manipur N\n",
      "./testdata/marathi/marathif10.mat\n",
      "marathi -> manipur N\n"
     ]
    }
   ],
   "source": [
    "tt,conf_mat,tot_spek=testing(test_data,feat_test,trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'manipur': 10, 'telugu': 9, 'bengali': 10, 'odia': 7, 'assam': 9, 'gujrati': 10, 'marathi': 10}\n",
      "Confusion Matrix:\n",
      " [[10.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  5.  3.  0.  0.  0.]\n",
      " [ 4.  0.  5.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  7.  0.  0.  0.]\n",
      " [ 0.  0.  0.  9.  0.  0.  0.]\n",
      " [ 7.  0.  0.  3.  0.  0.  0.]\n",
      " [ 8.  0.  1.  1.  0.  0.  0.]]\n",
      "Total Speakers: 7\n",
      "Accuracy:  33.84615384615385\n"
     ]
    }
   ],
   "source": [
    "print(tt)\n",
    "print(\"Confusion Matrix:\\n\",conf_mat)\n",
    "print(\"Total Speakers:\",tot_spek)\n",
    "print(\"Accuracy: \",(sum([ conf_mat[i][j] if i==j  else 0 for i in range(tot_spek) for j in range(tot_spek) ] )*100)/float(sum([i for i in tt.values()])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
