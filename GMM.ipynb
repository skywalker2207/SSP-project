{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import scipy.io.wavfile as wave\n",
    "from scipy.fftpack import fft, ifft, fftshift\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sklearn import mixture\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import json\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\CLG\\S3-1\\SSP\\SSP-project\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_mixtures = 7\n",
    "max_iterations = 200\n",
    "#calc_deltas=True\n",
    "#sr=8000\n",
    "#hop_length=int(0.005*sr)\n",
    "home_path = os.getcwd()\n",
    "#path = os.path.join(home_path, 'data')\n",
    "print(home_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_name):\n",
    "    feature = loadmat('file_name')\n",
    "    \n",
    "    dur_list = np.array([[element for element in upperElement] for upperElement in feature['dur']])\n",
    "    f0_list = np.array([[element for element in upperElement] for upperElement in feature['f0']])\n",
    "    amp_tilt_list = np.array([[element for element in upperElement] for upperElement in feature['amp_tilt']])\n",
    "    dur_tilt_list = np.array([[element for element in upperElement] for upperElement in feature['dur_tilt']])\n",
    "    differ_list = np.array([[element for element in upperElement] for upperElement in feature['differ']])\n",
    "    voiced_dur_list = np.array([[element for element in upperElement] for upperElement in feature['voiced_dur']])\n",
    "    log_energy_list = np.array([[element for element in upperElement] for upperElement in feature['log_energy']])\n",
    "    \n",
    "    dur_list = dur_list.ravel()\n",
    "    f0_list = f0_list.ravel()\n",
    "    amp_tilt_list = amp_tilt_list.ravel()\n",
    "    dur_tilt_list = dur_tilt_list.ravel()\n",
    "    differ_list = differ_list.ravel()\n",
    "    voiced_dur_list = voiced_dur_list.ravel()\n",
    "    log_energy_list = log_energy_list.ravel()\n",
    "    \n",
    "    feature_vec = np.vstack((dur_list,f0_list,amp_tilt_list,dur_tilt_list,differ_list,voiced_dur_list,log_energy_list))\n",
    "    return feature_vec\n",
    "    #print(feature_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_data_path,feat_train_path,trained_model_path):\n",
    "    all_speakers=glob.glob(train_data_path+'*')\n",
    "    #print(all_speakers)\n",
    "    print(2)\n",
    "\n",
    "    directory=feat_train_path\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "      \n",
    "    directory=trained_model_path\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    print(len(all_speakers))\n",
    "    for itr1 in range(0,len(all_speakers)):\n",
    "        \n",
    "        mats=glob.glob(all_speakers[itr1]+'/*.mat')\n",
    "        spk=(all_speakers[itr1]).split(\"/\")[-1]\n",
    "        #print((mats))\n",
    "\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "      \n",
    "        final_feat=np.empty([0, 35])\n",
    "        \n",
    "        for itr2 in range(0,len(mats)):\n",
    "  \n",
    "            #final_feat = eng.prosody(mats[itr2])\n",
    "            #mat file\n",
    "            mats[itr2]=mats[itr2].replace(\"\\\\\",\"/\")\n",
    "            print(mats[itr2])\n",
    "            final_feat = extract_features(mats[itr2])\n",
    "            print(final_feat.shape)\n",
    "\n",
    "        print(spk)    \n",
    "        np.savetxt(feat_train_path+spk+\"_all_features.txt\", final_feat, delimiter=\",\")\n",
    "\n",
    "        try:\n",
    "            gmm = mixture.GaussianMixture(n_components=n_mixtures, covariance_type='diag' , max_iter = max_iterations ).fit(final_feat)\n",
    "        except:\n",
    "            print(\"ERROR : Error while training model for file \"+spk)\n",
    "          \n",
    "        try:\n",
    "            joblib.dump(gmm,trained_model_path+spk+'.pkl')\n",
    "        except:\n",
    "            print(\"ERROR : Error while saving model for \"+spk)\n",
    "          \n",
    "\n",
    "    print(\"Training Completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(test_data_path,feat_test,trained_model_path):\n",
    "    # train feature extraction\n",
    "    all_speakers=glob.glob(test_data_path+'*')\n",
    "\n",
    "    import os\n",
    "    directory=feat_test\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    speakers = { all_speakers[k]:k for k in range(len(all_speakers)) }\n",
    "    print(speakers) \n",
    "   \n",
    "    num_test_cases={}\n",
    "    tct={}\n",
    "    for e in speakers:\n",
    "         num_test_cases[e.replace(test_data_path,'')]=len(os.listdir(e))-1\n",
    "         tct[e.replace(test_data_path,'')]=0\n",
    "\n",
    "    # print(num_test_cases)\n",
    "\n",
    "    spk_names = { all_speakers[k].replace(test_data_path,''):k for k in range(len(all_speakers)) }\n",
    "\n",
    "    total_speakers=len(num_test_cases)\n",
    "\n",
    "    confusion_matrix = np.zeros((total_speakers,total_speakers))\n",
    "\n",
    "\n",
    "    for itr1 in range(0,len(all_speakers)):\n",
    "        mats=glob.glob(all_speakers[itr1]+'/*.mat')\n",
    "        spk=(all_speakers[itr1]).split(\"/\")[-1]\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        final_feat=np.empty([[]])\n",
    "        output = {\"Test_i\":[],\"Accent\":[]}\n",
    "\n",
    "        for itr2 in range(0,len(mats)):\n",
    "            #print(wavs[itr2])\n",
    "\n",
    "            feat = extract_features(mats[itr2])\n",
    "            final_feat=np.concatenate((final_feat,feat),axis=0)\n",
    "            #print(final_feat.shape)\n",
    "            max_score=-np.inf\n",
    "            max_spk_name=\"\"\n",
    "\n",
    "            for modelfile in sorted(glob.glob(trained_model_path+'*.pkl')):\n",
    "                gmm = joblib.load(modelfile) \n",
    "                score=gmm.score(feat)\n",
    "                #print score\n",
    "                if score>max_score:\n",
    "                    max_score,max_spk_name=score,modelfile.replace(trained_model_path,'').replace('.pkl','')\n",
    "\n",
    "            print(spk+\" -> \"+max_spk_name+(\" Y\" if spk==max_spk_name  else \" N\"))\n",
    "            output [\"Test_i\"].append(mats[itr2])\n",
    "            output [\"Accent\"].append(max_spk_name)\n",
    "            confusion_matrix[ spk_names[spk] ][spk_names[max_spk_name]]+=1\n",
    "            tct[spk]+=1\n",
    "\n",
    "        #print(spk)\n",
    "        json_object = json.dumps(output, indent = 4)\n",
    "\n",
    "        with open(\"Haram.json\", \"w\") as outfile:\n",
    "              outfile.write(json_object)\n",
    "        np.savetxt(feat_test+spk+\"_all_features.txt\", feat, delimiter=\",\")\n",
    "        \n",
    "    return tct,confusion_matrix,total_speakers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "feat= 'D:/CLG/S3-1/SSP/SSP-project/feat/'\n",
    "feat_train= 'D:/CLG/S3-1/SSP/SSP-project/feat/train/'\n",
    "feat_test= 'D:/CLG/S3-1/SSP/SSP-project/feat/test/'\n",
    "trained_model= 'D:/CLG/S3-1/SSP/SSP-project/trained_model/'\n",
    "train_data= 'D:/CLG/S3-1/SSP/SSP-project/traindata/'\n",
    "test_data= 'D:/CLG/S3-1/SSP/SSP-project/testdata/'\n",
    "\n",
    "# for removing existing feature folders, models created\n",
    "if os.path.exists('D:/CLG/S3-1/SSP/SSP-project/feat/'):\n",
    "  !rm -rf  'D:/CLG/S3-1/SSP/SSP-project/feat/'\n",
    "if os.path.exists('D:/CLG/S3-1/SSP/SSP-project/feat/test/'):\n",
    "  !rm -rf  'D:/CLG/S3-1/SSP/SSP-project/feat/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "D:/CLG/S3-1/SSP/SSP-project/traindata/Assam/assamf1.mat\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'file_name.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_like, mode), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     41\u001b[0m     \u001b[39m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'file_name'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\CLG\\S3-1\\SSP\\SSP-project\\GMM.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/CLG/S3-1/SSP/SSP-project/GMM.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training(train_data,feat_train,trained_model)\n",
      "\u001b[1;32md:\\CLG\\S3-1\\SSP\\SSP-project\\GMM.ipynb Cell 14\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(train_data_path, feat_train_path, trained_model_path)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLG/S3-1/SSP/SSP-project/GMM.ipynb#X16sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     mats[itr2]\u001b[39m=\u001b[39mmats[itr2]\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLG/S3-1/SSP/SSP-project/GMM.ipynb#X16sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mprint\u001b[39m(mats[itr2])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CLG/S3-1/SSP/SSP-project/GMM.ipynb#X16sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     final_feat \u001b[39m=\u001b[39m extract_features(mats[itr2])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLG/S3-1/SSP/SSP-project/GMM.ipynb#X16sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mprint\u001b[39m(final_feat\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLG/S3-1/SSP/SSP-project/GMM.ipynb#X16sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(spk)    \n",
      "\u001b[1;32md:\\CLG\\S3-1\\SSP\\SSP-project\\GMM.ipynb Cell 14\u001b[0m in \u001b[0;36mextract_features\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CLG/S3-1/SSP/SSP-project/GMM.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_features\u001b[39m(file_name):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/CLG/S3-1/SSP/SSP-project/GMM.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     feature \u001b[39m=\u001b[39m loadmat(\u001b[39m'\u001b[39;49m\u001b[39mfile_name\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CLG/S3-1/SSP/SSP-project/GMM.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     dur_list \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[element \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m upperElement] \u001b[39mfor\u001b[39;00m upperElement \u001b[39min\u001b[39;00m feature[\u001b[39m'\u001b[39m\u001b[39mdur\u001b[39m\u001b[39m'\u001b[39m]])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CLG/S3-1/SSP/SSP-project/GMM.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     f0_list \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[element \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m upperElement] \u001b[39mfor\u001b[39;00m upperElement \u001b[39min\u001b[39;00m feature[\u001b[39m'\u001b[39m\u001b[39mf0\u001b[39m\u001b[39m'\u001b[39m]])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:224\u001b[0m, in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[39mLoad MATLAB file.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m variable_names \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mvariable_names\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 224\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    225\u001b[0m     MR, _ \u001b[39m=\u001b[39m mat_reader_factory(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    226\u001b[0m     matfile_dict \u001b[39m=\u001b[39m MR\u001b[39m.\u001b[39mget_variables(variable_names)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[0;32m    134\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[0;32m    136\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m@contextmanager\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     f, opened \u001b[39m=\u001b[39m _open_file(file_like, appendmat, mode)\n\u001b[0;32m     18\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m         \u001b[39myield\u001b[39;00m f\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:45\u001b[0m, in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m appendmat \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m file_like\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.mat\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     44\u001b[0m         file_like \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.mat\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 45\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_like, mode), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m     48\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mReader needs file name or open file-like object\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     49\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'file_name.mat'"
     ]
    }
   ],
   "source": [
    "training(train_data,feat_train,trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt,conf_mat,tot_spek=testing(test_data,feat_test,trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tt)\n",
    "print(\"Confusion Matrix:\\n\",conf_mat)\n",
    "print(\"Total Speakers:\",tot_spek)\n",
    "print(\"Accuracy: \",(sum([ conf_mat[i][j] if i==j  else 0 for i in range(tot_spek) for j in range(tot_spek) ] )*100)/float(sum([i for i in tt.values()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
