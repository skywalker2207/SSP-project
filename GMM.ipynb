{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import scipy.io.wavfile as wave\n",
    "from scipy.fftpack import fft, ifft, fftshift\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sklearn import mixture\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import json\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/skywalker/ssp/SSP-project\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_mixtures = 7\n",
    "max_iterations = 200\n",
    "#calc_deltas=True\n",
    "#sr=8000\n",
    "#hop_length=int(0.005*sr)\n",
    "home_path = os.getcwd()\n",
    "#path = os.path.join(home_path, 'data')\n",
    "print(home_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file):\n",
    "    feature = loadmat(file)\n",
    "    \n",
    "    dur_list = np.array([[element for element in upperElement] for upperElement in feature['dur']])\n",
    "    f0_list = np.array([[element for element in upperElement] for upperElement in feature['f0']])\n",
    "    amp_tilt_list = np.array([[element for element in upperElement] for upperElement in feature['amp_tilt']])\n",
    "    dur_tilt_list = np.array([[element for element in upperElement] for upperElement in feature['dur_tilt']])\n",
    "    differ_list = np.array([[element for element in upperElement] for upperElement in feature['differ']])\n",
    "    voiced_dur_list = np.array([[element for element in upperElement] for upperElement in feature['voiced_dur']])\n",
    "    log_energy_list = np.array([[element for element in upperElement] for upperElement in feature['log_energy']])\n",
    "    \n",
    "    dur_list = dur_list.ravel()\n",
    "    f0_list = f0_list.ravel()\n",
    "    amp_tilt_list = amp_tilt_list.ravel()\n",
    "    dur_tilt_list = dur_tilt_list.ravel()\n",
    "    differ_list = differ_list.ravel()\n",
    "    voiced_dur_list = voiced_dur_list.ravel()\n",
    "    log_energy_list = log_energy_list.ravel()\n",
    "    \n",
    "    feature_vec = np.vstack((dur_list,f0_list,amp_tilt_list,dur_tilt_list,differ_list,voiced_dur_list,log_energy_list))\n",
    "    feature_vec = np.nan_to_num(feature_vec)\n",
    "    feature_vec = feature_vec.T\n",
    "    return feature_vec\n",
    "    #print(feature_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_data_path,feat_train_path,trained_model_path):\n",
    "    all_speakers=glob.glob(train_data_path+'*')\n",
    "    #print(all_speakers)\n",
    "    print(2)\n",
    "\n",
    "    directory=feat_train_path\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "      \n",
    "    directory=trained_model_path\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    print(len(all_speakers))\n",
    "    for itr1 in range(0,len(all_speakers)):\n",
    "        \n",
    "        mats=glob.glob(all_speakers[itr1]+'/*.mat')\n",
    "        spk=(all_speakers[itr1]).split(\"/\")[-1]\n",
    "        #print((mats))\n",
    "\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "      \n",
    "        final_feat=np.empty([0, 35])\n",
    "        \n",
    "        for itr2 in range(0,len(mats)):\n",
    "  \n",
    "            #final_feat = eng.prosody(mats[itr2])\n",
    "            #mat file\n",
    "            #mats[itr2]=mats[itr2].replace(\"\\\\\",\"/\")\n",
    "            #print(mats[itr2])\n",
    "            final_feat = extract_features(mats[itr2])\n",
    "            #print(final_feat.shape)\n",
    "\n",
    "        #print(spk)    \n",
    "        np.savetxt(feat_train_path+spk+\"_all_features.txt\", final_feat, delimiter=\",\")\n",
    "\n",
    "        try:\n",
    "            gmm = mixture.GaussianMixture(n_components=n_mixtures, covariance_type='diag' , max_iter = max_iterations ).fit(final_feat)\n",
    "        except:\n",
    "            print(\"ERROR : Error while training model for file \"+spk)\n",
    "          \n",
    "        try:\n",
    "            joblib.dump(gmm,trained_model_path+spk+'.pkl')\n",
    "        except:\n",
    "            print(\"ERROR : Error while saving model for \"+spk)\n",
    "          \n",
    "\n",
    "    print(\"Training Completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(test_data_path,feat_test,trained_model_path):\n",
    "    # train feature extraction\n",
    "    all_speakers=glob.glob(test_data_path+'*')\n",
    "\n",
    "    import os\n",
    "    directory=feat_test\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    speakers = { all_speakers[k]:k for k in range(len(all_speakers)) }\n",
    "    print(speakers) \n",
    "   \n",
    "    num_test_cases={}\n",
    "    tct={}\n",
    "    for e in speakers:\n",
    "        num_test_cases[e.replace(test_data_path,'')]=len(os.listdir(e))-1\n",
    "        tct[e.replace(test_data_path,'')]=0\n",
    "\n",
    "    # print(num_test_cases)\n",
    "\n",
    "    spk_names = { all_speakers[k].replace(test_data_path,''):k for k in range(len(all_speakers)) }\n",
    "    total_speakers=len(num_test_cases)\n",
    "    confusion_matrix = np.zeros((total_speakers,total_speakers))\n",
    "\n",
    "\n",
    "    for itr1 in range(0,len(all_speakers)):\n",
    "        mats=glob.glob(all_speakers[itr1]+'/*.mat')\n",
    "        spk=(all_speakers[itr1]).split(\"/\")[-1]\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            \n",
    "        final_feat=np.empty([0, 35])\n",
    "        output = {\"Test_i\":[],\"Accent\":[]}\n",
    "\n",
    "        for itr2 in range(0,len(mats)):\n",
    "            print(mats[itr2])\n",
    "\n",
    "            feat = extract_features(mats[itr2])\n",
    "            #print(feat)\n",
    "            max_score=-np.inf\n",
    "            max_spk_name=\"\"\n",
    "\n",
    "            for modelfile in sorted(glob.glob(trained_model_path+'*.pkl')):\n",
    "                gmm = joblib.load(modelfile) \n",
    "                score=gmm.score(feat)\n",
    "                #print score\n",
    "                if score>max_score:\n",
    "                    max_score,max_spk_name=score,modelfile.replace(trained_model_path,'').replace('.pkl','')\n",
    "\n",
    "            print(spk+\" -> \"+max_spk_name+(\" Y\" if spk==max_spk_name  else \" N\"))\n",
    "            output [\"Test_i\"].append(mats[itr2])\n",
    "            output [\"Accent\"].append(max_spk_name)\n",
    "            confusion_matrix[ spk_names[spk] ][spk_names[max_spk_name]]+=1\n",
    "            tct[spk]+=1\n",
    "\n",
    "        #print(spk)\n",
    "        json_object = json.dumps(output, indent = 4)\n",
    "\n",
    "        with open(\"Haram.json\", \"w\") as outfile:\n",
    "              outfile.write(json_object)\n",
    "        np.savetxt(feat_test+spk+\"_all_features.txt\", feat, delimiter=\",\")\n",
    "        \n",
    "    return tct,confusion_matrix,total_speakers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./feat/\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "feat= './feat/'\n",
    "feat_train=  './feat/train/'\n",
    "feat_test= './feat/test/'\n",
    "trained_model= './trained_model/'\n",
    "train_data= './traindata/'\n",
    "test_data= './testdata/'\n",
    "print(feat)\n",
    "# for removing existing feature folders, models created\n",
    "if os.path.exists('./feat/'):\n",
    "  !rm -rf  './feat/'\n",
    "if os.path.exists('./feat/test/'):\n",
    "  !rm -rf  './feat/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "Training Completed\n"
     ]
    }
   ],
   "source": [
    "training(train_data,feat_train,trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'./testdata/Assam': 0, './testdata/Marathi': 1, './testdata/Telugu': 2}\n",
      "./testdata/Assam/assamf40.mat\n",
      "Assam -> Telugu N\n",
      "./testdata/Assam/assamm30.mat\n",
      "Assam -> Telugu N\n",
      "./testdata/Assam/assamf41.mat\n",
      "Assam -> Telugu N\n",
      "./testdata/Assam/assamf42.mat\n",
      "Assam -> Telugu N\n",
      "./testdata/Assam/assamm29.mat\n",
      "Assam -> Telugu N\n",
      "./testdata/Assam/assamm32.mat\n",
      "Assam -> Telugu N\n",
      "./testdata/Assam/assamm34.mat\n",
      "Assam -> Telugu N\n",
      "./testdata/Assam/assamm31.mat\n",
      "Assam -> Telugu N\n",
      "./testdata/Assam/assamm27.mat\n",
      "Assam -> Telugu N\n",
      "./testdata/Assam/assamm36.mat\n",
      "Assam -> Telugu N\n",
      "./testdata/Assam/assamf37.mat\n",
      "Assam -> Telugu N\n",
      "./testdata/Assam/assamf36.mat\n",
      "Assam -> Telugu N\n",
      "./testdata/Assam/assamm26.mat\n",
      "Assam -> Telugu N\n",
      "./testdata/Assam/assamf38.mat\n",
      "Assam -> Telugu N\n",
      "./testdata/Assam/assamf39.mat\n",
      "Assam -> Telugu N\n",
      "./testdata/Assam/assamm35.mat\n",
      "Assam -> Telugu N\n",
      "./testdata/Assam/assamm33.mat\n",
      "Assam -> Telugu N\n",
      "./testdata/Marathi/marathim37.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathim44.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathif42.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathif45.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathim41.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathif39.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathif40.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathim47.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathif36.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathif38.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathim46.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathif43.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathim45.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathim40.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathim39.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathim43.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathif41.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathif44.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathim38.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathif37.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathim36.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Marathi/marathim42.mat\n",
      "Marathi -> Telugu N\n",
      "./testdata/Telugu/telugu38.mat\n",
      "Telugu -> Telugu Y\n",
      "./testdata/Telugu/telugu35.mat\n",
      "Telugu -> Telugu Y\n",
      "./testdata/Telugu/telugu32.mat\n",
      "Telugu -> Telugu Y\n",
      "./testdata/Telugu/telugu40.mat\n",
      "Telugu -> Telugu Y\n",
      "./testdata/Telugu/telugu31.mat\n",
      "Telugu -> Telugu Y\n",
      "./testdata/Telugu/telugu36.mat\n",
      "Telugu -> Telugu Y\n",
      "./testdata/Telugu/telugu39.mat\n",
      "Telugu -> Telugu Y\n",
      "./testdata/Telugu/telugu41.mat\n",
      "Telugu -> Telugu Y\n",
      "./testdata/Telugu/telugu34.mat\n",
      "Telugu -> Telugu Y\n",
      "./testdata/Telugu/telugu33.mat\n",
      "Telugu -> Telugu Y\n"
     ]
    }
   ],
   "source": [
    "tt,conf_mat,tot_spek=testing(test_data,feat_test,trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Assam': 17, 'Marathi': 22, 'Telugu': 10}\n",
      "Confusion Matrix:\n",
      " [[ 0.  0. 17.]\n",
      " [ 0.  0. 22.]\n",
      " [ 0.  0. 10.]]\n",
      "Total Speakers: 3\n",
      "Accuracy:  20.408163265306122\n"
     ]
    }
   ],
   "source": [
    "print(tt)\n",
    "print(\"Confusion Matrix:\\n\",conf_mat)\n",
    "print(\"Total Speakers:\",tot_spek)\n",
    "print(\"Accuracy: \",(sum([ conf_mat[i][j] if i==j  else 0 for i in range(tot_spek) for j in range(tot_spek) ] )*100)/float(sum([i for i in tt.values()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
